{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import tokenizer\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for loading dataset\n",
    "CSV_PATH = \"../data/tbrain_train_final_0610.csv\"\n",
    "NEWS_PATH = os.path.abspath(\n",
    "    f\"../data/news\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "def create_dataset(csv_path=None, news_path=None, max_seq_len=512, seed=None, tokenizer=None):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    name_list = df[\"name\"].tolist()\n",
    "    news_idx = df[\"news_ID\"].tolist()\n",
    "    name_list = [ast.literal_eval(name) for name in name_list]\n",
    "\n",
    "    news = sorted(os.listdir(news_path))\n",
    "    news.sort(key=len, reverse=False)\n",
    "    news = [f\"{news_path}/{path}\" for path in news if path.endswith(\".txt\")]\n",
    "\n",
    "    corpus = []\n",
    "\n",
    "    for i, p in enumerate(news):\n",
    "        with open(p, \"r\") as f:\n",
    "            text = f.readlines()\n",
    "            text = [line.strip('\\n') for line in text]\n",
    "            corpus.append(' '.join(text))\n",
    "\n",
    "    # Filter for used sentences\n",
    "    key_sentences = []\n",
    "    for doc, n in zip(corpus, name_list):\n",
    "        tmp = []\n",
    "        c = [s for s in doc.split(\"。\")]\n",
    "        have_name = False\n",
    "        for s in c:\n",
    "            for name in n:\n",
    "                if name in s:\n",
    "                    have_name = True\n",
    "                    tmp.append(s)\n",
    "                    break\n",
    "        if have_name == True:\n",
    "            cleaned.append(\"。\".join(tmp))\n",
    "        else:\n",
    "            cleaned.append(doc)\n",
    "\n",
    "    cleaned = tokenizer.clean(key_sentences)\n",
    "    tokens = tokenizer.tokenize(cleaned)\n",
    "    labels = tokenizer.labeler(name_list, tokens)\n",
    "    encoded = tokenizer.encode(tokens)\n",
    "\n",
    "    dataset = list(zip(encoded, labels, name_list))\n",
    "\n",
    "    # Drop data without document\n",
    "    dropped = []\n",
    "    for idx, data in enumerate(dataset):\n",
    "        if data[0] != []:\n",
    "            if len(data[0]) > max_seq_len:\n",
    "                dropped.append([data[0][:max_seq_len], data[1][:max_seq_len], data[2]])\n",
    "            else:\n",
    "                dropped.append(data)\n",
    "\n",
    "    print(f\"# of data: {len(dropped)}\")\n",
    "\n",
    "    train_data, test_data = train_test_split(dropped, test_size=0.1, random_state=seed)\n",
    "\n",
    "    with open(\"./data/train_L.pickle\", \"wb\") as fb:\n",
    "        pickle.dump(train_data, fb)\n",
    "\n",
    "    with open(\"./data/test_L.pickle\", \"wb\") as fb:\n",
    "        pickle.dump(test_data, fb)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/5023 [00:00<01:16, 65.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Document Length (before): 56511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5023/5023 [01:15<00:00, 66.57it/s]\n",
      "  4%|▎         | 176/5023 [00:00<00:02, 1757.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Document Length (after): 56511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5023/5023 [00:03<00:00, 1583.91it/s]\n",
      "100%|██████████| 5023/5023 [00:00<00:00, 68609.28it/s]\n",
      "100%|██████████| 5023/5023 [00:00<00:00, 40163.85it/s]\n",
      "100%|██████████| 5023/5023 [08:44<00:00,  9.58it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of data: 4914\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.Tokenizer()\n",
    "train_data, test_data = create_dataset(CSV_PATH, NEWS_PATH, 512, 9, t)\n",
    "\n",
    "with open(\"./data/tokenizer.pickle\", \"wb\") as fb:\n",
    "    pickle.dump(t, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
